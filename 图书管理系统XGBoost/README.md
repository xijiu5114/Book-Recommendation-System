梯度提升树（Gradient Boosting Decision Tree, GBDT）基础XGBoost是基于梯度提升树的优化实现，其核心思想是迭代构建多棵决策树，每棵树通过拟合前序模型的残差（预测误差）逐步优化整体模型。通过加法模型（Additive Model）将弱学习器（决策树）组合成强学习器，目标是最小化损失函数并控制模型复杂度。
（1）原理基础：基于梯度提升决策树（GBDT）框架，通过迭代生成多棵决策树（CART树）来逐步优化模型预测结果。其核心思想是加法模型，即最终预测结果为所有树预测值的加权和，通过最小化损失函数和正则化项来控制模型复杂度。
（2）核心步骤：
迭代生成决策树：对于每轮迭代m，计算目标函数的梯度如下图
![image](https://github.com/user-attachments/assets/5b2351a3-2d8c-4e56-a302-1b42536d6f46)

再选择最优分裂节点，遍历所有特征和阈值，以增益（Gain）最大化为标准，其中γ是分裂最小增益阈值，防止过拟合。
![image](https://github.com/user-attachments/assets/a9c9be96-b210-4741-b43c-fc7a0f7fccb0)

剪枝与正则化：预剪枝：通过最大树深度（max_depth）、叶子节点最小样本数（min_child_weight）限制树复杂度；后剪枝：基于增益阈值合并低贡献节点。
组合模型输出：最终预测结果为所有树的加权和。
